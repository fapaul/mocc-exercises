CPU benchmarks questions:
1. Look at linpack.sh and linpack.c and shortly describe how the benchmark
works.

linpack.sh checks if a excuable linpack is existing if not it compiles tries to copile it from linepack.c.
This probably dont work out of the box inside a windows systems.

Next the system root is used to decide if the bash script is used in windows or not and the regarding executable is called.
The output is captured and the result is taken from a know position of the output.

linpack.c runs 3 functions in a loop an benchmarks them. (nreps)
The program allocates a 2D array and provide this to the functions:
    matgen - Fills the array with data
    dgefa - factors a double precision matrix by gaussian elimination
    dgesl - solves the double precision system a * x = b  or  trans(a) * x = b

While only dgefa and dgesl are benchmarked by measuring the execution time. (tdgefa, tdgesl)
The operations are can be computed by the size of the 2D array. (ops)

kflops are then calculate by:  kflops=2.*nreps*ops/(1000.*(tdgefa+tdgesl));

2. Find out what the LINPACK benchmark measures (try Google). Would you
expect paravirtualization to affect the LINPACK benchmark? Why?

Linpack measures the needed time to solve the equitation and divides it by the FLOPS needed.
If the excution time is higher the measured kflops becomes smaller.
Paravirtualization will effect the benchmark.
Because it measures the kflops for the program not for the cpu.
In paravirtualization the cpu can be shared between multiple guest systems.
If more guest systems share the cpu linpack will share the cpu and the time to complete will be longer.
Therefore the kflops will become smaller.

3. Look at your LINPACK measurements. Are they consistent with your
expectations? If not, what could be the reason?

Google:
    max	2219414.402 kflops
    min	 943616.87 kflops
AWS:
    max 1207363.522 kflops
    min  117376.02 kflops

The min kflops has factor of 2 compared to the max kflops.
This is consistent with our expection.
Because auf paravirtualization our instance shares the hardware with other instances.
So on high workload for all instances the throughput for a instance can drop.
Nevertheless the factor 2 is quite high, because this means the program took double the time finish.
For ec2 the difference is significant larger between the min and max.

------

Memory benchmarks questions:
1.Find out how the memsweep benchmark works by looking at the shell script
and the C code. Would you expect virtualization to affect the memsweep
benchmark? Why?

memsweep.sh works same as linpack.sh besides it takes the memsweep.c file instead of the linpack.c and the echoed value is captured from another position.

memsweep.c allocates an array of size 8096 * 4096.
Then it measures the time for iterate 10 times over the array,
reassigns all the values from another field of the array and 
frees the array.
The measured time is then returned in second.

We expect virtualization effects the benchmark.
Because it adds additional indirection while accessing memory.
Also the time is meassured, therefore if the cpu is shared the execution time will also increase.

2.. Look at your memsweep measurements. Are they consistent with your
expectations? If not, what could be the reason?

Google:
    max 38.999 seconds
    avg 35.54 seconds
    min 7.719 seconds

AWS: 
    max 60.25 seconds
    avg 13.544 seconds
    min  5.847 seconds

The most parts are consistent with our expectations. 
But the factor 5 between max and min is a bit higher then we are expected.
In comparison to the cpu benchmark where the ec2 was outperformed by google the memsweep test shows better write rates on ec2.

------

Disk benchmarks questions:

Google:
    sequential:
        max 36 mb/s
        min 20.5 mb/s
    random: 
        Can't measure because of fio error only on google cloud.
        See description [1] below .
AWS:
    sequential:
        max 39 mb/s
        min 28.5 mb/s
    random:
        max 2415 IOPS
        min 752 IOPS

1.Look at the disk measurements. Are they consistent with your expectations? If
not, what could be the reason?

The sequential write and read has a factor of 2 between min and max.
We expected this behavior because multiple instance could share the same disk, which leads to shared bandwitdh.

The above described behavior is also shown by the random write benchmark since the phyical might be used by different machine in the same time which would result in additional overhead.

2. Compare the results for the two operations (sequential, random). What are
reasons for the differences?

Specifically for random operations the shared disk seems to be performance overhead. This might result due to the position of the disk head and potentially big movements. In contrary to the sequential operation where the head doesn't jump after finding the inital write position.


------
[1]
Information to missing Fio results not working on Google Instance!

Always same error:
fio: job startup hung? exiting.
fio: 1 job failed to start

fio: terminating on signal 15

Tried several options, including only reading or writing, minimising threads, ...
Most of which worked on Linux Docker Instance
